#### 1. 학습 날짜 // 2020-06-26(금)

#### 2. 학습시간 // 9:00~22:00

#### 3. 학습 범위 및 주제 // 빅데이터 처리 프로세스 및 모니터링

#### 4. 동료 학습 방법 // -

#### 5. 학습 목표 // 빅데이터 처리 프로세스 및 모니터링 방법에 대해 대략적으로 파악한다.

---

#### 6. 상세 학습 내용

# Big Data를 위한 데이터 모니터링 특강

## 소개

연사: 현재 Andrew Park 팔로알토 모니터링스(기업 보안 1티어 기업). 원래 링크드인에서 데이터 엔지니어로 14년 일함. 



## 전체 이해

#### Big Data의 시작

-  Legacy **Small Data**는 이미 존재. Legacy에서 확장된 개념.
-  데이터는 4차 산업혁명시대의 '쌀' 혹은 '원유'
-  스마트폰, 5G, IoT 등의 출현으로 데이터의 기하급수적 증가.
-  **Batch** processing ==> **Real-time** streaming data processing으로 발전함.

#### 농업 vs Big Data 

- 빅데이터는 농사와 일맥상통한다!
- 쌀 생산과정 = Big Data 생산과정
- 땅크기 증가 = 수확량 증가(Linear)

|      | 농업                                                         | Big Data                                                     |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 전통 | 소규모 영농, 자급자족 형태의 농업<br />개인 영농으로 소와 인력을 이용 | MB, GB급 소규모 데이터 처리. <br />On-Perm에서 1-2대의 서버 운영 |
| 현대 | 대규모 경작지와 기계화와 영농<br />트랙터와 비행기 등을 활용한 기업형 영농 | TB, PB, EX급 대단위 데이터 처리. <br />2000년부터 등장한 온라인 기업들 중심 |

#### 쌀 생산단계 (빅데이터 생산단계와 매칭됨)

1. 댐을 지어서 물 확보

2. 댐에서 저수지로 물을 옮기는 수로공사, 논의 모양과 크기를 표준화하고 기계화가 가능하도록 변경
3. 벼농지 인근에 여러 크기의 **저수지**를 조성하여 물을 저장
   - 물은 댐에서 연결된 수로로 공급되도록 조성
   - 여러 저수지의 물 취수, 배수 통로를 만들어서 사용이 용이하게 만듦
4. 지은 쌀을 도정
   - 벼 도정과 이물질 제거
   - 쌀 상품화 작업 -패키징
   - 쌀 생산자 추적 시스템

#### 빅데이터 생산단계

1. 데이터를 모으기 위한 **수집 저장공간** 마련
   - 필요 용량 만큼의 **컴퓨터를 노드 연결, 대용량 저장소**를 구성.
   - 모든 데이터를 한곳에 모아 놓는 **'Data Lake'** 로 표현.
   - e.g. Hadoop, BigQuery, Casandra, Mongo 등
2. 우리 몸의 **혈관**과 같이 데이터를 다른 곳으로 보내기 위한 데이터 채널.
   - 데이터의 전달이 늦어지면, 자체적으로 데이터를 보관.
   - 데이터의 **형식/포맷 (Schema)**을 일정하게 만들어서 데이터 전달.
   - e.g. Apache Kafka, Pub/Sub, Active MQ, AWS KInesis(Kafka를 기반으로 만듦) 등. 아파치 카프카를 제일 많이 쓰고 이걸 링크드인에서 만들어서 오픈소스화했음. 
3. Data Warehouse를 구성하여 실제 데이터 분석, ML 등에 사용할 수 있게 구성.
   - 데이터 용도별 유입되는 데이터를 pre-compute를 통해 지속적인 Summary 데이터를 만들어서 필요한 데이터를 제공.
   - 카프카에서는 파티션 개념
   - e.g. MySQL, Elasticsearch(쿼리가 많이 빨라진다), Druid(로우데이터 추출에 쉬움), Pinot(링크드인이 리딩하는 오픈소스. 실시간 이용자 데이터 수집할 때 많이 씀), Terradata, AWS Redshift, Oracle.. 각각 용도에 맞게 적합한게 조금씩 다르다.
4. 데이터의 **Noise**를 제거
   - 최종 단계의 데이터 가공 **Visualization**을 통한 데이터 의미를 찾는 작업
   - e.g. Tableau, R, Jupyter Notebook, 심지어 Excel
   - 데이터의 생산, 가공, 처리, 전송, 사용에 이르는 단계를 **Metadata를 통해 관리**

#### 쌀의 활용

| 농업                                                         | 빅데이터                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 쌀을 활용한 음식들<br /> 댐은 수력발전소에서 전기 생산<br />전기를 이용한 다양한 활용분야 | 전기를 이용한 AI 자율주행차량<br />스마트 팩토리<br />공공의료 등 |

#### 빅데이터 고려사항

- 데이터 전체볼륨

- 단계별 모델에 의한 Summary하는 방법

- 데이터 처리 Logic에 의한 데이터의 Partition

- #### Data science and analysis on data warehouse

- Avoid unnecessary resource-expensive processing steps whenever possible

- 생산량 증대를 위해 농약 사용하듯 컴퓨터 파워를 늘리면 좋지만 돈이 든다.

- 자연재해처럼 해킹이 쉽게 당할 수 있다.

- 새로운 농법 도입처럼 새로운 기술 도입

## 빅데이터 & 모니터링 기술

상황별 앤드류의 체감

- 2012
  - **Hadoop**
  - Cloudera
  - HBase
  - Casandra
  - Big Query
  - Mongo
  - NoSQL
  - 이런 데이터를 모으는 기술들이 엄청나게 발전했다.
- 2013
  - **Map Reduce**
  - Pig (Map reduce의 쉬운 버전)
  - Tajo (지금은 망해서 없어짐. 한국 스타트업. 잘나갔었다고 함ㅠ)
  - Hive
  - 모은 데이터의 의미를 추츨하는 기술들
- 2014
  - **Processing** (어떻게 빨리빨리 처리할까) 이 때부터 데이터 활용 폭발함
  - Kafka
  - Samza
  - Spark
  - Storm
- 2015
  - **Visualization** (데이터를 어떻게 활용을 할까)
  - Tableau
  - R
  - Zeppeline
  - Jupyter notebook
- 2017
  - **Metadata**
  - GDPR
  - WhereHow
- 2018
  - **AI**
  - Tensorflow
  - Torch
  - ThirdEyes
  - ...
  - Kubernetes

#### Big Data stacks in layers

| 클라우드     | Ingestion | Stream Processing | Storage | Presentataion |
| ------------ | --------- | ----------------- | ------- | ------------- |
| 애져         |           |                   |         |               |
| AWS          |           |                   |         |               |
| 구글클라우드 |           |                   |         |               |
| 오픈소스     |           |                   |         |               |

많이쓰는걸 볼드로 처리함. 너무 데이터가 많아지면 비싸서 오픈소스로 만들어서 쓰게되더라.

#### Data Lake

요즘은 다들 데이터레이크를 만들어서 쓰는 추세임. 한국전체를 따져도 이걸 하는 사람이 500명이 안될듯?

링크드인 초당 2백만 데이터.. 데이터 넘나 크니까 빅데이터는 사실 A에서 B로 데이터를 옮기는게 주 업무가 된다.

데이터 다루는 사람들이 데이터에 접근해서 쓸 수 있게 하는 것!

#### Real-tiem streaming

- **실시간으로 들어오는 데이터를 처리**하기 위한 기술.
- 실시간으로 들어오는 데이터를 그냥 Data Lake에 저장과 함께 실시간으로 **변환하는 기술**
- 데이터를 변환하면서 **실시간으로 Data Warehouse에 바로 전송**도 가능.

예를들어서 구글에서 현대 쏘나타를 치면 페이스북 광고가 전부 차광고가 되어버림. 옛날에는 최소 하루는 걸렸었는데, 지금은 실시간으로 데이터 변환하고, 규칙정하고, 이벤트 로그를 날리기 때문에 가능.

- 개발된 ML 모델들을 적용하여 실시간으로 **AI관련 기술들을 적용**
- Frameworks: Samza, Storm, Spark, Streaming, Flink, Dataflow 등.

처음 리얼타임으로 받아오는 것은 kafka를 많이 씀. 스타트업할 때는 가격이 부담되어서 Redis를 이용하기도 함.

#### Data Dashboard - metrics visualization

데이터를 어떻게 보느냐에 따라서 그 의미가 바뀔 수 있기 때문에 아주아주 중요한 영ㅇㄱ

- **Key Metrics**를 쉽게 볼 수 있도록 구성.
- Weekly Active User(WAU), Monthly Active User(MAU), Week of Week(WoW), CAC: 한명의 고객을 유치하는데 든 비용(시계열별로), Customer Churn Rate(돈을 더이상 안내는 사람들, 떠나는 비율)
- Single Page Dashboard로 구성.
- Top management 부터 모든 구성원이 사용할 수 있도록 만들어야 데이터 중심의 회사 문화로 변경 가능. 링크드인의 경우 진짜 무슨 얘기하려고 하면 데이터 어딨어! 얘기부터 나옴. 데이터가 없으면 얘기를 못한다.
- 사용자가 직접 Report조건을 바꿔서 데이터를 Explore 할 수 있는 기능이 필요.

- Apache Jupyter
- Apache Zeppelin
- Tableau

#### Data visualization (UX/UI)

- D3.js, Hightchars, Googlecharts 등의 Javascript Libraries를 통해 개발.

왠만하면 Hightcharts면 충분하더라~!

어떤 단어가 많이 쓰이는지 보려면 그루핑하거나 버블차트로 보면 딱 옮.

Tableau: 유료 프로그램, 많이 웹브라우저와 연동이 가능.

#### 농업 vs Big Data in monitoring

- 데이터 수가 부족하진 않은지, 거버넌스가 똑같은 룰로 들어가야하는데 룰셋이 잘 되고 있는지, 중간에 에러가 나지는 않았는지 등등
- 시간대별로 감시가 되어야함.

#### Prometheus + Grafana outline

- 인더스트리 표준 느낌임!
- 처리량은 얼마나 늘었는지, 카프카에 걸려있는 랙은 얼마인지, 큐잉은 얼마인지, 서버의 CPU는 얼마인지, 
- 지연시간은 너무 크면 상위 10퍼센트만 떼어서 본다. 이게 SLO인듯
- Dimensional data model: 데이터를 한 줄로 쫙 늘어놓는 것.
- PromQL: 프로메테우스에서 제공하는 SQL 같은 것. 서브쿼리랑 조인은 없는데 이건 룰셋으로 대신 구현 가능.
- 프로메테우스 그라파나 조합이 쿠버네티스로 할 때 굉장히 연동이 잘 됨.
- 실제 사용 예시
  - Visualize: 날짜별로 찍어서 들어가고 시간별로 레인지를 보는 경우
  - Dynamic dashboard: 데이터가 들어오면 다이나믹 대시보드로 시간별로 데이터를 짤라서 볼 수 있음. 
  - Metric explore : 이건 오픈소스에는 없고, 돈내면 있음. 데이터 2개 3개를 겹쳐서 비교해볼 수 있음.
  - Loki explore: 중간중간에 문제가 있는 스파이크가 있음. 그럼 그 때 왜 스파이크가 발생했는지 알아야한다. 플로그인 loki-ops 끼우면 로우데이터와 함께 볼 수 있어서 좋음.
  - Alerting: 특정 조건을 벗어나면 알림이 오도록 할 수 있다.
- 구현 구조
  - ...

#### Prometheus HTTP client

카운터: 계속 늘어나는거

게이지: 늘었다 주었다하는거

서머리: 요약데이터

히스토그램: 퍼센트 P50, P90, P99를 주로 씀. P100은 넘 레어한 케이스 버리기 위해 잘 안씀.

#### Big Data Tech Stacks for Startups

| 단계                 | 추천                                                         |
| -------------------- | ------------------------------------------------------------ |
| 데이터수집           | Gaggle, data.gov(왠만한 데이터 다 있음.), buzzfeed(뉴스)     |
| 데이터저장           | Mongo 많이 씀. <br /> ELK(Elasticsearch + Logstash + Kibana)<br />엘라스틱서치는 저장용도로는 별로임. 대신 검색용도나 특정 서비스용도로 많이 씀.  우버의 경우 누가 근처에 있는지를 찾아주는 기능이 있음. |
| 데이터처리, 머신러닝 | Python, Pytorch, Tensflow 가볍게 시작해도 된다. 굳이 자바까지 안가도 고랭, php, 파이썬만해도 충분함. 보통 데이터를 A에서 B로 옮기는 것. |
| 데이터화면           | 쥬피터 노트북. 굳이 웹사이트하려면 리액트 같은거 써야함.     |
| 서버 구성            | 도커로 올리면 된다. 쿠버네티스로 클라우드 써서해도 된다.     |

사견) 하둡은 생각보다 느리고 비싸서 완전 꿈의 머신은 아니다.

## 링크드인 적용 기술 소개

링크드인은 데이터 관련해서 매우 큰 회사고, 링크드인 출신이 여러곳을 꽉잡고 있다.

#### 스쿱

어떤 얘기를 많이 하는지

어떤 잡이 열려있는지

어떤 배경들이 있는 사람들로 이뤄져있는지

#### 기대와 현실 @LinkedIn

첨에는 데이터 사이언티스트 자체가 별로 없어서 뭘해도 잘했음.

지금은 다들 고도화되어서 뭘해도 시간은 오래걸리고 투자대비 효율이 떨어짐;



- 자동화실현: 백단에서는 데이터셋 정의하면 사용까지 쫙 처리됨.
- 데이터 거버넌스(데이터 기준 표준화)를 통해 데이터하나하나의 출처 확인 가능, 무결성.
- 플랫폼을 **재사용**가능하게 만듦.
- 인하우스로 개발 원핢.
- Kafka Schema를 통해 일괄처리.



- 투자대비 효율이 저하됨.
- 인하우스 퍼스트라서, 오픈소스 사용이 잘 안됨.
- 모든 사람들이 승진하려고 티나는 일만 하려고 함.
- 굳이 자바를 안써도 될 것, 자바를 굳이쓰려고 하고, 굳이 카프카를 쓰려고 한다. 때문에 속도가 떨어진다.

- 이미 시스템이 갖춰져있어서 실험이 어렵다. 하려면 설득을 몇달을 해야한다.
- 애드훅 같은거 추가 힘들다. 이미 다 자동화되어있기 때문이다.
- 리뷰절차가 너무 복잡해짐(자동화, 데이터 거버넌스..) 때문에 개발 속도가 현저하게 느려지는 문제가 생김. 일주일이면 할 것을 한달을 해야됨.
- 데이터 정의 책임자와 참여 개발자간에 이해가 충돌해서 석달씩 싸우고 그런다고 함. 이건 어쩔 수 없음. 데이터가 커지니까.

#### LinkedIn Data Processing

페타바이트를 테라바이트로 줄이고 이걸 기가바이트로 줄이고 메가바이트로 줄임. 메가바이트 정도로 줄어들어야 소비자에게 의미있는 의미가 나온다.

옆에 나오는건 링크드인이 오픈소스를 리딩하고 있는 곳임.

왜 친구신청이 3000명이상 안될까? 3000명이상 친구연결되면 큰 문제가 생긴다. 3000명꺼를 계속 계산해서 A에게 포스트를 보여줄지말지를 계속 계산해야해서 너무 비용이 크다.

#### Visualization - Raptor

데이터를 앞에 정의하면 바로 화면이 이렇게 나오도록 자동화되어있음.

데이터를 쿼리처럼 어디서 나오는 걸 어떻게 보겠다 정의. 권한도 설정.

#### Personalized Viz. for Priority Level

빨간색이 떠날 가능성 큰 고객이고, 클릭하면 어떻게 관리해야하는지 뜨도록 함. 세일즈나 마케팅에 있는 분들이 쉽게 관리할 수 있도록함.

#### Data driven business by metrics

매일 아침에 출근해서 1시간 가량 자신이 개발한 제품의 메트릭을 보는데 시간을 쓴다. 만약 이 메트릭이 죽으면 모든 관련있는 사람들이 와서 해결하는데 달려들어야한다. 너무 복잡하게 엉켜있기 때문에.

모든 데이터사이언티스트들이 겪는 문제이다.

이 문제를 해결해주는 것은 하둡에 있는 데이터에 메타데이터를 붙여가지고 문제가 발생하면 바로바로 문제를 발견할 수 있도록 한다.

#### Graph - Monitoring

서버의 가용성, 런타임, 리스폰스 코드. 
버튼을 특정 위치에 넣은 다음에 모니터링 해서 A/B 테스트를 할 수 있도록 실험하고 결정한다.

#### Confront Big Data

- 데이터 모니터링이 없으면 써먹지 못함. 제대로된 데이터인지 알 수 없기 때문에.
- 추천엔진은 나중에 개발하는게 맞다. 서비스로 일정수준의 고객확보가 된다음에 해야 맞는다. 초기에는 그 문제를 24시간 고민한 창업자가 추천해야함. 추천엔진은 정말정말 나중에.
- 데이터가 1TB 미만으로 작으면 MySQL, Mongo로 작게 시작해라.
- Hadoop은 정말 많은 컴퓨터가 필요하고 노력과 돈과 전문기술이 많이 들어간다.
- B2C에 적용시, **가설->실험->결과** 분석에 이르는 단계 필요.
- Avoid A-Z in-House build, Leverage to use the **Open sources**
- 음성이나 이미지 처리가 아니면 Tensflow 없어도 시작이 가능하다. 특히 텍스트.
- 최소 한 회사의 **Cloud**를 알면 좋을 듯.
- **생활코딩**을 통해 Big Data를 하기에는 너무 방대하다. 쉽지않다.



#### TO be continue

- NoSQL 데이터베이스 디자인, 리얼타임 스트리밍, 카프카 등등
- 데이타팀 어떻게 만드는지(Science, Analysis, Engineering)
- 데이터 거버넌스 어떻게 관리
- 데이터 비정상인 것 어떻게 아는지
- 실험 A/B 테스트는 어떻게 하는 거인지
- 메타데이터 처리
- 온프레미스 vs 클라우드 in Big Data

## Q&A

#### 사용한 PPT 자료를 공유할 수 있는지?

공개해보겠다.

#### Q1) MySQL처럼 축소해서 쓸 수 있는 Hadoop의 대용으로 쓸 수 있는 것이 있을까?

정형화되지 않은 데이터를 담을 때는 하둡에 넣으면 좋은데 넣기도 어렵고 비싸다.

MongoDB를 추천한다. 그냥 때려넣고 쓰기 딱 좋다. 

MySQL은 한계가 명확하다.

카산드라 같은 것은 코딩량이 너무 많고 고려해야할게 너무 많은데, MongoDB는 그렇지 않다.

캐시 같은거 구현하려면 Redis를 이용하면 좋을 것 같다.

빠르고 다른 곳으로 데이터 옮기기도 좋다.

#### Q2) mySQL에서 다른 디비로 이전통합 해보신 경험이 있으시다면,,? (백업과 리스크매니징, 검증 작업 경험이 있으시면)

옮기기 좋게 툴이 넘나 많이 나와있다. 

리스크 검증은 크론잡등으로 직접해야함.

#### Q4) 구글 클라우드를 언급하셨는데 연습 또는 독학할 때에 FireBase와 연동이 가능한지, DB의 경우에는 MariaDB 사용이 가능한지 궁금합니다.

물론임. 실리콘밸리는 풀스크랩을 많이씀. 통계에 유리해서. FireBase는 DB 신경안쓰고 앱 개발할 때 넘나 좋음. 구글클라우드는 엔터프라이즈로 가면 한계가 명확하고 하나부터 끝까지 다 만들어야해서 비추천이다. 대신 사용자 편리성과 저렴함이 강점이다. 스타트업이나 크지 않은 기업은 가능.

#### Q5) [질문] 퍼블릭 클라우드 벤더사가 많고 제공하는 기능도 유사한데 어떤 기준으로 선택하면 될까요?

개인적으로 가격. 커지면 가격이 장난이 아니기 때문이다. 아마존은 스토리지가 비싸고 컴퓨팅이 쌈. 구글클라우드는 반대임. 프로모션 많이 하는 곳에서 하는 것도 괜찮을듯.

#### Q6) 빅데이터에 대한 인식이 많이 달라졌습니다. 특히 빅데이터라고 했을때 MB, GB가 아닌 TB, PB이상의 데이터가 나와야 빅데이터를 할 수 있다고 생각했습니다. 이 부분에 대해서 PB, TB가 아닌 GB=>MB에서 의미를 찾을 수 있다라는 이야기에 인식의 전환이 되었습니다. MB에서 빅데이터의 의미가 해석될 수 있다?라고 말씀을 해주셨는데, MB, GB 데이터만 있어도 되는건가요? 아니면 TB와 같은 엄청난 데이터가 존재하고 MB로 데이터를 나눠야되는건가요?

데이터 접근하고 정리하는데 비용을 줄이기 위해 쪼개고, 의미를 추출하기 위해서 줄이는 것이 중요하다.

#### Q7) 저는 소프트웨어 창업에 관심이 많은 학생인데요 창업을 하려고 한다면 강사님께서 보시기에 가장 필요하다고 보시는 덕목? 같은 것이 궁금합니다.

투자자 관점에서 봤을 때 성장해야하듯. 돈에 대한 욕망이 있어야한다고 생각한다.

#### Q8) [질문] 클라우드 서비스를 이용하는 사업이나 큰 회사에서 내부 데이터를 활용하는 상황이 아니면 사실 데이터 자체를 구할 수 있는 방법이 거의 불가능(보안때문에도)에 가까운데 이런 경우에는 빅데이터 기반의 Ai 관련 솔루션이나 서비스를 개발하기 어려움이 있는데 이런 어려움은 없으셨나 해서요. 좋은 방법이 혹시 있는지...

데이터는 직접 모아야한다. 이미지는 구글에서 크롤링도 할 수 있고. 캐글이나 공공데이터 활용해야함. Go.k

예를들어 아마존에서 가격비교를 할 때 모든 사이트를 크롤링해서 만드는 가격임.

#### Q9) 백업구성은?

서버 8개 중 3~4개는 백업용으로 남겨둬서 4개 망가져도 백업용으로 할 수 있도록함. 장애에 대처하도록 하는게 중요함.

다른 강의 있다니까 참고~ 

7월 3일은 "빅데이터 분석으로 코로나19로 바뀌는 모바일 라이프 스타일" 강좌가 있음.
https://forms.gle/WfYc4H1rR9ArpTfY9

---

#### 학습 내용에 대한 개인적인 총평

평소 관심있었던 빅데이터에 대해서 좋은 무료 강의가 있어서 참여했습니다. 아직 관련공부가 부족한만큼 강의가 너무 어려울까봐 걱정스러웠는데 상당히 쉽게 진행할 수 있어서 만족스러웠습니다. 전체적으로 어떤 방식으로 움직이는지 확실히 파악한 전문가가 본질을 쉽게쉽게 설명해내는 느낌..! 지식을 전달한다면 이렇게 전달해야겠다는 생각이 들었습니다. ft_services에서 다루는 쿠버네티스는 그렇다쳐도 그라파나도 현업에서 활발하게 쓴다기에 42seoul 커리큘럼이 적절하다는 느낌을 받았습니다.

---

#### 다음 학습 계획

- minishell readline 구현